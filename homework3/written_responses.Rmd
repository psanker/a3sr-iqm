---
title: "Homework 3"
subtitle: "IQM"
date: "`r strftime(Sys.Date(), format = '%Y-%m-%d')`"
author: Patrick Anker
header-includes:
output: 
  bookdown::pdf_document2:
    includes:
      in_header: "../preamble.tex"
    toc: false
    extra_dependencies: "float"
---

```{r setup1, include=FALSE, cache=FALSE}
knitr::read_chunk(here::here("homework3/analysis.R"))
```

```{r preamble, include=FALSE}
```

# Question 1 {-}

Generally, the outcomes $Y_i$ in a linear model are not identically distributed because there is a generating function of the predictors that affect the outcomes. Question 3 is a great example where each individual $Y_i$ has a different distribution. However, if there was no relation of the predictors to the outcome (i.e. all coefficients to the predictors are zero), then each $Y_i$ would be identically distributed as there is no underlying generating function.

Instead, we care about the distribution of the *residuals* of the model fit. If a model is a good fit, then the residuals should be identically distributed ("homoscedastic"). If the residuals are *not* identically distributed ("heteroscedastic"), then the model may have too many or too few predictors.

# Question 2 {-}

```{r, echo=FALSE, out.width="55%", fig.align="center", fig.cap="Theoretical periodic data"}
knitr::include_graphics(here::here("homework3/q2.jpg"))
```

The sketch I provided examines some theoretical data measurements of current in milliamperes versus time in seconds. In some cases, this could be a linear relationship (or stepwise) if the current is operating in a DC domain, which would imply a constant or linearly changing voltage. However, given that the data oscillates back and forth, a linear model would not suffice. Instead, a sinusoidal model would be better in the alternating current domain.

# Question 3 {-}

```{r, echo=FALSE, out.width="40%", fig.align="center", fig.cap="Linear model against integral inputs", fig.pos="H"}
knitr::include_graphics(here::here("homework3/q3.jpg"))
```

This sketch jitters the data points, which ought to be sampled from the set $\{1, 2, 3\}$, so that the uncertainty can be displayed.

```{r question3, echo=FALSE, fig.cap="Histogram of $y$ values at large $N$", fig.align="center", out.width="45%"}
```

Even though the input data are integers, $y$ will eventually have a Normal distribution due to the perturbations in the data. These small deviations form a sort of random walk which generalizes at large $N$ to the Normal distribution. We can empirically confirm this with the histogram shown in Fig. \@ref(fig:question3). At $N=10,000$ we can still see three peaks from the original integral data, but already the bell curve is starting to take shape. This is a consequence of the Central Limit Theorem.

# Question 4 {-}

```{r, echo=FALSE, out.width="55%", fig.align="center", fig.cap="Sketch of the relationship between midterm grades and final grades", fig.pos="H"}
knitr::include_graphics(here::here("homework3/q4.jpg"))
```

# Question 5 {-}

See `analysis.R` for the definition of the function `sim_and_plot()`.

```{r question5, echo=FALSE}
```

```{r q5f1, echo=FALSE, fig.cap="Code evaluation: does $\\{a, b\\}$=0 create uncorrelated noise?", out.width="55%", fig.align="center"}
```

```{r q5f2, echo=FALSE, fig.cap="Examining the effect of changing the variance -- larger $s$ values expectedly increase the point spread", fig.align="center", fig.subcap=c("Smaller variance", "Larger variance"), fig.ncol=2, out.width="45%", fig.pos="H"}
```

Fig. \@ref(fig:q5f1) checks to make sure that the code renders a flat line in the trivial case. There is a slight negative slope, but on the whole it does seem that there is no relation between $x$ and $y$. More interestingly, Fig. \@ref(fig:q5f2) examines the effect of increasing the variance $\sigma^2$ on both the scatterplot and the regression. 

```{r q5f2-compare, echo=FALSE}
```

```{r q5t1, echo=FALSE, fig.pos="H"}
kableExtra::kable(
  tab_1,
  digits = 3,
  caption = "Comparison of model uncertainty",
  format = "latex",
  booktabs = TRUE
) |>
  kableExtra::kable_styling()
```

Visually, it's clear that there's more uncertainty due to the way these data are spread out, but numerically it's hard to see without looking at the model outputs. Tab. \@ref(tab:q5t1) shows that (b), with the higher variance, has more uncertainty in both the intercept and slope estimates.

```{r q5f3, echo=FALSE, fig.cap="Examining the effect of changing the observation count -- smaller $n$ values expectedly make the fit less stable", fig.align="center", fig.subcap=c("Smaller observation count", "Larger observation count"), fig.ncol=2, out.width="45%"}
```

```{r q5f3-compare, echo=FALSE}
```

```{r q5t2, echo=FALSE}
kableExtra::kable(
  tab_2,
  digits = 3,
  caption = "Comparison of model uncertainty",
  format = "latex",
  booktabs = TRUE
) |>
  kableExtra::kable_styling()
```

Finally, Fig. \@ref(fig:q5f3) and Tab. \@ref(tab:q5t2) examine the effect of changing the number of observations $n$. While the resulting slopes are similar, it's clear from Tab. \@ref(tab:q5t2) that there is a 3-4x increase in uncertainty in the estimates of both the intercept and slope.

# Question 6 {-}

```{r question6, include=FALSE}
```

```{r q6f1, echo=FALSE, fig.cap="Estimates of intercept and slope improve with more observations", fig.align="center", fig.subcap=c("Intercept", "Slope"), fig.ncol=2, out.width="50%"}
```

As Fig. \@ref(fig:q6f1) shows, the more observations there are to use as model inputs, the more certain the model becomes of its estimates. I provided quantile lines as well to highlight the narrowing quality of the scatterplot as $n$ increases.

```{r q6f2, echo=FALSE, fig.cap="Std. errors of intercept and slope improve with more observations", fig.align="center", fig.subcap=c("Intercept", "Slope"), fig.ncol=2, out.width="50%"}
```

Fig. \@ref(fig:q6f2) highlights that the standard errors have a different characteristic curve as $n$ increases. This is to be expected as the standard error should scale $\sim\ n^{-1/2}$.

# Extra Credit {-}

```{r extra-credit-a, include=FALSE}
```

```{r extra-credit-b, echo=FALSE, fig.cap="(b) Total number of shots per simulation appears to follow a Poisson distribution", out.width="55%", fig.align="center"}
```

Fig. \@ref(fig:extra-credit-b) shows that the total number of shots appears to follow a Poisson distribution. We'd expect that the uncertainty would be $\sqrt{\lambda}$ where $\lambda$ is the mean. Instead, on average, the player makes `r report_uncertainty(n_shots)` shots per simulation. Thus, these data are *not* generated by a Poisson process.

```{r extra-credit-c, echo=FALSE, fig.cap="(c) Apparent cumulative density function structure comparing total shots to proportion successful", out.width="55%", fig.align="center"}
```

Fig. \@ref(fig:extra-credit-c) has a peculiar shape. The points are jittered due to the discrete behavior of the data; what's more interesting is the apparent trend in the data. It looks like the points are distributed over a CDF.
